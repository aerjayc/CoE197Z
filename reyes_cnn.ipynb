{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0928 03:57:41.570663 34712 deprecation_wrapper.py:119] From c:\\users\\marcus\\anaconda3\\envs\\coe197z\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0928 03:57:41.585592 34712 deprecation_wrapper.py:119] From c:\\users\\marcus\\anaconda3\\envs\\coe197z\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0928 03:57:41.651417 34712 deprecation_wrapper.py:119] From c:\\users\\marcus\\anaconda3\\envs\\coe197z\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0928 03:57:41.670397 34712 deprecation_wrapper.py:119] From c:\\users\\marcus\\anaconda3\\envs\\coe197z\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0928 03:57:41.681338 34712 deprecation.py:506] From c:\\users\\marcus\\anaconda3\\envs\\coe197z\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0928 03:57:41.721230 34712 deprecation_wrapper.py:119] From c:\\users\\marcus\\anaconda3\\envs\\coe197z\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0928 03:57:42.976206 34712 deprecation_wrapper.py:119] From c:\\users\\marcus\\anaconda3\\envs\\coe197z\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0928 03:57:43.240531 34712 deprecation_wrapper.py:119] From c:\\users\\marcus\\anaconda3\\envs\\coe197z\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0928 03:57:43.352233 34712 deprecation.py:323] From c:\\users\\marcus\\anaconda3\\envs\\coe197z\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 1.6909 - acc: 0.4255\n",
      "10000/10000 [==============================] - 1s 92us/step\n",
      "-------- 0 --------\n",
      "Accuracy:  53.290000000000006\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 1.3026 - acc: 0.5418\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "-------- 1 --------\n",
      "Accuracy:  59.760000000000005\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 1.1451 - acc: 0.5922\n",
      "10000/10000 [==============================] - 1s 82us/step\n",
      "-------- 2 --------\n",
      "Accuracy:  60.25\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 1.0503 - acc: 0.6286\n",
      "10000/10000 [==============================] - 1s 82us/step\n",
      "-------- 3 --------\n",
      "Accuracy:  63.690000000000005\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.9759 - acc: 0.6562\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "-------- 4 --------\n",
      "Accuracy:  68.72\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.9269 - acc: 0.6717\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "-------- 5 --------\n",
      "Accuracy:  69.59\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.8812 - acc: 0.6876\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "-------- 6 --------\n",
      "Accuracy:  69.57\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.8570 - acc: 0.7005\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "-------- 7 --------\n",
      "Accuracy:  70.08\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.8220 - acc: 0.7092\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "-------- 8 --------\n",
      "Accuracy:  68.82000000000001\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.7994 - acc: 0.7186\n",
      "10000/10000 [==============================] - 1s 84us/step\n",
      "-------- 9 --------\n",
      "Accuracy:  70.07\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.7812 - acc: 0.7242\n",
      "10000/10000 [==============================] - 1s 82us/step\n",
      "-------- 10 --------\n",
      "Accuracy:  72.52\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.7548 - acc: 0.7317\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "-------- 11 --------\n",
      "Accuracy:  73.02\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 259us/step - loss: 0.7419 - acc: 0.7355\n",
      "10000/10000 [==============================] - 1s 82us/step\n",
      "-------- 12 --------\n",
      "Accuracy:  71.94\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 0.7276 - acc: 0.7436\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "-------- 13 --------\n",
      "Accuracy:  72.38\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 0.7079 - acc: 0.7516\n",
      "10000/10000 [==============================] - 1s 82us/step\n",
      "-------- 14 --------\n",
      "Accuracy:  72.27\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.6926 - acc: 0.7548\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "-------- 15 --------\n",
      "Accuracy:  70.26\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.6847 - acc: 0.7577\n",
      "10000/10000 [==============================] - 1s 82us/step\n",
      "-------- 16 --------\n",
      "Accuracy:  74.27\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.6724 - acc: 0.7613\n",
      "10000/10000 [==============================] - 1s 87us/step\n",
      "-------- 17 --------\n",
      "Accuracy:  74.36\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.6599 - acc: 0.7664\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "-------- 18 --------\n",
      "Accuracy:  70.67\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.6457 - acc: 0.7706\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "-------- 19 --------\n",
      "Accuracy:  74.64\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s 268us/step - loss: 0.6412 - acc: 0.7717\n",
      "10000/10000 [==============================] - 1s 84us/step\n",
      "-------- 20 --------\n",
      "Accuracy:  72.39999999999999\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s 279us/step - loss: 0.6329 - acc: 0.7750\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "-------- 21 --------\n",
      "Accuracy:  74.33\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.6253 - acc: 0.7777\n",
      "10000/10000 [==============================] - 1s 85us/step\n",
      "-------- 22 --------\n",
      "Accuracy:  75.08\n",
      "Final Accuracy:  75.08\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#Deprecated replaced with keras.datasets\n",
    "#For CS231N data loading\n",
    "import os\n",
    "import platform\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense,Activation,BatchNormalization,Dropout,Flatten,Conv2D\n",
    "\n",
    "from keras.optimizers import adam\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float')\n",
    "X_test = X_test.astype('float')\n",
    "y_train = y_train.astype('float')\n",
    "y_test = y_test.astype('float')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "kernel_size = 3\n",
    "filters = 64\n",
    "strides = (2,2)\n",
    "dropout = 0.3\n",
    "input_shape = (32,32,3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = filters,\n",
    "                kernel_size = kernel_size,\n",
    "                activation = 'relu',\n",
    "                strides = strides,\n",
    "                input_shape = (32,32,3)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters = filters*2,\n",
    "                kernel_size = kernel_size,\n",
    "                strides = strides,\n",
    "                activation = 'relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters = filters*4,\n",
    "                kernel_size = kernel_size,\n",
    "                activation = 'relu',\n",
    "                strides = strides))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "num_epochs = 50\n",
    "for i in range(num_epochs):\n",
    "    model.fit(X_train, y_train, epochs = 1, batch_size = batch_size)\n",
    "\n",
    "    score = model.evaluate(X_test, y_test, batch_size = batch_size)\n",
    "    print(\"--------\",i,\"--------\")\n",
    "    print(\"Accuracy: \",(100.0 * score[1]))\n",
    "    if(score[1] > 0.75):\n",
    "        break\n",
    "print(\"Final Accuracy: \",(100.0 * score[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harder to play around on CNN because of the relatively long training times\n",
    "#But using the general settings of MLP and strided convolution.\n",
    "#More filters seem to be better\n",
    "#Trying to see if I can run 64 as batchsize -update 32 seems to work better as I still can't beat 74.58\n",
    "#Also did what Sir mentioned in class as a good rule of thumb doubling filtersize per \"layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eitherway CNN massively outperforms MLP on less epochs as well\n",
    "#I forgot which youtube video it was I got it from but I believe the handwavy explanation for this is\n",
    "#that CNN preserves the structure of the data(image) thus allowing for more humanlike interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
