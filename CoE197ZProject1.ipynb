{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fromm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeglEVvP0TUk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a5ca8a1-3e03-48ff-efab-645f3b67909d"
      },
      "source": [
        "###Reyes, Marcus Group 7\n",
        "\n",
        "###CoE 197Z Project 1.1\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,Dropout,BatchNormalization\n",
        "from keras import regularizers\n",
        "from keras.optimizers import adam,sgd\n",
        "from keras import optimizers\n",
        "\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import string\n",
        "\n",
        "#from sixfunctions import load_train, load_x_test, clean_data_with_mean\n",
        "!pip install category_encoders"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqG88I49TdBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "dac8813e-8130-4c0d-dfe6-a4fbe880284c"
      },
      "source": [
        "from importlib import reload\n",
        "import sixfunctions\n",
        "sixfunctions = reload(sixfunctions)\n",
        "from sixfunctions import load_train, load_x_test"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.21.3)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.3.1)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.16.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phFfzI3u0ppG",
        "colab_type": "code",
        "outputId": "5f4caf58-5c0a-49b7-b18a-295fe44e9165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "#[reason] / dup == duplicate\n",
        "                  \n",
        "unique_cols = { 'funder',       # 1898 unique values\n",
        "                'ward',         # 2092 unique values\n",
        "                'installer',    # 2146 unique values\n",
        "                'scheme_name',  # 2697 unique values\n",
        "                'subvillage',   # 19288 unique values\n",
        "                'wpt_name' }    # 37400 unique values\n",
        "\n",
        "do_not_include = {'quality_group',      # exact duplicate of 'quality'\n",
        "                  'recorded_by',        # uniform value\n",
        "\n",
        "                  'wpt_name',           #unique\n",
        "                  'subvillage',         #dup of loc, numerous\n",
        "                  'scheme_name',        #messy numerous\n",
        "                  'payment',            #duplicate of payment type\n",
        "                  'quantity_group',     #Dup of quantity\n",
        "                  'waterpoint_type_group',#dup of wtptype less data\n",
        "                  'source_type',        #dup of source\n",
        "                  'extraction_type',    #dup of extr_type_group\n",
        "                  #'extraction_type_group',                 \n",
        "                  'region'} - unique_cols#enc in regcode\n",
        "\n",
        "#These are sort of ordinal data\n",
        "do_not_one_hot = {'id','gps_height','doy_recorded_sin','doy_recorded_cos',\n",
        "                  'longitude','latitude','population','amount_tsh'} | unique_cols\n",
        "\n",
        "clean_up = {'population','amount_tsh'}\n",
        "\n",
        "#Tentatively do not include.\n",
        "do_not_include_tent = {'funder','installer','ward','lga'} - unique_cols\n",
        "\n",
        "#Don't know what it means \n",
        "do_not_include_temp = {'num_private'} - unique_cols\n",
        "\n",
        "\n",
        "x, train_col,y = load_train(\"train_set_values.csv\",\"train_set_labels.csv\",\n",
        "                            do_not_include, do_not_one_hot,\n",
        "                            clean_up, do_not_include_tent, do_not_include_temp,\n",
        "                            unique_cols)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "About to clean up\n",
            "About to clean up\n",
            "one-hot encoding:\n",
            "\tbasin:\t9 categories\n",
            "\tregion_code:\t27 categories\n",
            "\tdistrict_code:\t20 categories\n",
            "\tpublic_meeting:\t2 categories\n",
            "\tscheme_management:\t13 categories\n",
            "\tpermit:\t2 categories\n",
            "\tconstruction_year:\t55 categories\n",
            "\textraction_type_group:\t13 categories\n",
            "\textraction_type_class:\t7 categories\n",
            "\tmanagement:\t12 categories\n",
            "\tmanagement_group:\t5 categories\n",
            "\tpayment_type:\t7 categories\n",
            "\twater_quality:\t8 categories\n",
            "\tquantity:\t5 categories\n",
            "\tsource:\t10 categories\n",
            "\tsource_class:\t3 categories\n",
            "\twaterpoint_type:\t7 categories\n",
            "\tyear_recorded:\t5 categories\n",
            "BINARY ENCODING:\n",
            "\t subvillage \t 19288  unique values  -> 15  columns\n",
            "\t funder \t 1898  unique values  -> 11  columns\n",
            "\t installer \t 2146  unique values  -> 12  columns\n",
            "\t scheme_name \t 2697  unique values  -> 12  columns\n",
            "\t ward \t 2092  unique values  -> 12  columns\n",
            "\t wpt_name \t 37400  unique values  -> 16  columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xdZr_LgOZfY",
        "colab_type": "code",
        "outputId": "b9c39b66-7e66-45a8-99d2-09878533fad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "###Model\n",
        "\n",
        "hidden = 1024\n",
        "dropout = 0.4\n",
        "\n",
        "(trash, input_dim) = x.shape\n",
        "\n",
        "activation = 'relu'\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Dense(hidden, input_dim = input_dim))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Activation(activation))\n",
        "\n",
        "model.add(Dense(hidden, input_dim = input_dim))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Activation(activation))\n",
        "\n",
        "model.add(Dense(hidden, input_dim = input_dim))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Activation(activation))\n",
        "\n",
        "\n",
        "model.add(Dense(3,input_dim = hidden))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "#history = model.fit(x, y, epochs=20, batch_size=4096*16, validation_split=0.2)\n",
        "history = model.fit(x, y, epochs=20, batch_size=256, validation_split=0.2)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 47520 samples, validate on 11880 samples\n",
            "Epoch 1/20\n",
            "47520/47520 [==============================] - 4s 81us/step - loss: 0.6812 - acc: 0.7136 - val_loss: 0.6059 - val_acc: 0.7572\n",
            "Epoch 2/20\n",
            "47520/47520 [==============================] - 3s 60us/step - loss: 0.5914 - acc: 0.7582 - val_loss: 0.5725 - val_acc: 0.7699\n",
            "Epoch 3/20\n",
            "47520/47520 [==============================] - 3s 60us/step - loss: 0.5467 - acc: 0.7772 - val_loss: 0.5500 - val_acc: 0.7699\n",
            "Epoch 4/20\n",
            "47520/47520 [==============================] - 3s 60us/step - loss: 0.5135 - acc: 0.7905 - val_loss: 0.5491 - val_acc: 0.7756\n",
            "Epoch 5/20\n",
            "47520/47520 [==============================] - 3s 60us/step - loss: 0.4874 - acc: 0.7986 - val_loss: 0.5305 - val_acc: 0.7853\n",
            "Epoch 6/20\n",
            "47520/47520 [==============================] - 3s 59us/step - loss: 0.4619 - acc: 0.8093 - val_loss: 0.5445 - val_acc: 0.7817\n",
            "Epoch 7/20\n",
            "47520/47520 [==============================] - 3s 60us/step - loss: 0.4439 - acc: 0.8158 - val_loss: 0.5228 - val_acc: 0.7895\n",
            "Epoch 8/20\n",
            "47520/47520 [==============================] - 3s 59us/step - loss: 0.4207 - acc: 0.8270 - val_loss: 0.5297 - val_acc: 0.7882\n",
            "Epoch 9/20\n",
            "26624/47520 [===============>..............] - ETA: 1s - loss: 0.3958 - acc: 0.8361"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-fae70ce0cf74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#history = model.fit(x, y, epochs=20, batch_size=4096*16, validation_split=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ2-AmxsWhSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "max_score = -1\n",
        "iter_of_max = 0,0\n",
        "train_plot = []\n",
        "val_plot = []\n",
        "x_axis = []\n",
        "\n",
        "#Save the weights for k-fold validation\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "amount = 120\n",
        "k_folds = 5\n",
        "\n",
        "###When i use k_folds instead of 10 for the np.zeros initialization it acts up\n",
        "val_plot = np.zeros((k_folds,amount))\n",
        "train_plot = np.zeros((k_folds,amount))\n",
        "test_plot = np.zeros((k_folds,amount)) \n",
        "x_axis = np.zeros((k_folds,amount))\n",
        "total = int(x.shape[0])\n",
        "for j in range(k_folds):\n",
        "\n",
        "    whole = np.arange(0,total)\n",
        "    test_range = np.arange((j)*(total/k_folds), (j)*(total/k_folds)+(total/k_folds),dtype = 'int')\n",
        "    \n",
        "    train_range = np.delete(whole, test_range)\n",
        "    \n",
        "    \n",
        "    #K-fold validation setup\n",
        "    x_train = x[train_range,:]\n",
        "    x_pretest = x[test_range,:]\n",
        "    y_train = y[train_range,:]\n",
        "    y_pretest = y[test_range,:]\n",
        "    \n",
        "    for i in range(amount):\n",
        "\n",
        "        history = model.fit(x_train, y_train, epochs = 1, batch_size = 4096*16, verbose = 0)\n",
        "        score = model.evaluate(x_pretest, y_pretest, batch_size = 512, verbose = 0)\n",
        "\n",
        "        if  float(100 * score[1]) > float(max_score):\n",
        "            max_score = float(100 * score[1])\n",
        "            iter_of_max = i,j\n",
        "        if i%10 == 0:\n",
        "            print(\"----------\",i,\"-\",j,\"-----------\")\n",
        "            print(\"Test accuracy: \", (100.0 * score[1]))\n",
        "            print(\"Maxscore: \", max_score, \"at\", iter_of_max,\"epoch-kthfold\")\n",
        "        \n",
        "        #for the plot\n",
        "        x_axis[j,i] = i\n",
        "        test_plot[j,i] = (score[1])\n",
        "        train_plot[j,i] = np.array(history.history['acc'])\n",
        "\n",
        "        \n",
        "\n",
        "    if j == k_folds - 1:\n",
        "        break\n",
        "    model.load_weights('model.h5')\n",
        "    print(\"Reloading Model\")\n",
        "    \n",
        "print(\"Done evaluating performance\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}