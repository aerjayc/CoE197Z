{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoE197ZProject1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aqW4K2fE2M9k"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeglEVvP0TUk",
        "colab_type": "code",
        "outputId": "aea48125-6c70-4b87-b95f-1175f448fbd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "###Reyes, Marcus Group 7\n",
        "\n",
        "###CoE 197Z Project 1.1\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,Dropout,BatchNormalization\n",
        "from keras import regularizers\n",
        "from keras.optimizers import adam,sgd\n",
        "from keras import optimizers\n",
        "\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import string\n",
        "\n",
        "#from sixfunctions import load_train, load_x_test, clean_data_with_mean\n",
        "!pip install category_encoders"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.16.5)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.3.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.21.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.5.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.0)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E85Pye_xzFX",
        "colab_type": "text"
      },
      "source": [
        "#### Under the hood"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqG88I49TdBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from numpy import genfromtxt\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "#Given the dataframe and the column it replaces all zero nonavailable values with the mean\n",
        "def clean_data_with_mean(data, column):\n",
        "    replace_map = {column:{0:np.nan}}\n",
        "    data.replace(replace_map, inplace=True)\n",
        "\n",
        "    mean = data[column].mean()\n",
        "    data[column] = data[column].fillna(0)\n",
        "    replace_map = {column:{0:mean}}\n",
        "    data.replace(replace_map, inplace=True)\n",
        "    return data\n",
        "\n",
        "def date_to_cyclic(data):\n",
        "    # For date_recorder, convert to three columns, 2 for cyclical day of year recorded and the year recorded\n",
        "    # https://www.avanwyk.com/encoding-cyclical-features-for-deep-learning/\n",
        "    data[\"date_recorded\"] = pd.to_datetime(data[\"date_recorded\"], format=\"%Y-%m-%d\")\n",
        "    data[\"day_of_year_recorded\"] = data[\"date_recorded\"].apply(lambda x: x.timetuple().tm_yday)\n",
        "\n",
        "    data[\"year_recorded\"] = data[\"date_recorded\"].apply(lambda x: x.year)\n",
        "    data[\"doy_recorded_sin\"] = data[\"day_of_year_recorded\"].apply(lambda x: np.sin(2 * np.pi * x/365.0))\n",
        "    data[\"doy_recorded_cos\"] = data[\"day_of_year_recorded\"].apply(lambda x: np.cos(2 * np.pi * x/365.0))\n",
        "    data = data.drop([\"date_recorded\", \"day_of_year_recorded\"], axis='columns')\n",
        "\n",
        "    return data\n",
        "\n",
        "def onehot_encode(train, test, cols):\n",
        "    print(\"one-hot encoding:\")\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    for col in cols:\n",
        "        print(f\"\\t{col}:\", end='\\t')\n",
        "\n",
        "        train[col] = train[col].fillna(\"\")\n",
        "        test[col]  = test[col].fillna(\"\")\n",
        "\n",
        "        encoder = OneHotEncoder(sparse=False, categories='auto', handle_unknown='ignore')\n",
        "        onehot_cols = encoder.fit_transform(train[col].to_numpy().reshape(-1,1))\n",
        "        categories = encoder.categories_[0]\n",
        "        col_names = [f\"{col}_{cat}\" for cat in categories]  # names of new onehot columns\n",
        "\n",
        "        onehot_cols = pd.DataFrame(onehot_cols, columns=col_names)\n",
        "        train = pd.concat([train.drop([col], axis='columns'), onehot_cols], axis='columns')\n",
        "\n",
        "        onehot_cols = encoder.transform(test[col].to_numpy().reshape(-1,1))\n",
        "        onehot_cols = pd.DataFrame(onehot_cols, columns=col_names)\n",
        "        test = pd.concat([test.drop([col], axis='columns'), onehot_cols], axis='columns')\n",
        "        \n",
        "        print(str(len(categories)) + \" categories\")\n",
        "    return train, test\n",
        "\n",
        "def nominal_to_binary(train, test, cols):\n",
        "    print(\"binary encoding:\")\n",
        "    #!pip install category_encoders\n",
        "    from category_encoders import BinaryEncoder\n",
        "    import math\n",
        "    for col in cols:\n",
        "        n_unique = len(train[col].unique())\n",
        "        print('\\t', col, end=\" \")\n",
        "        print('\\t', n_unique, \" unique values\", \" -> \", end='')\n",
        "        print(math.ceil(math.log(n_unique, 2)), \" columns\")\n",
        "\n",
        "        encoder = BinaryEncoder(verbose=1, cols=[col])\n",
        "        train = encoder.fit_transform(train)\n",
        "        test  = encoder.transform(test)\n",
        "\n",
        "        i = 0\n",
        "        while(f\"col_{i}\" in train.columns):\n",
        "            rename_format = {f\"col_{i}\": f\"{col}_col_{i}\"}\n",
        "            train = train.rename(columns=rename_format)\n",
        "            test  = test.rename(columns=rename_format)\n",
        "            i += 1\n",
        "    return train, test\n",
        "\n",
        "def preprocess_data(train, labels, test, drop_cols=None, unique_cols=None,\n",
        "                    binary_cols=None, onehot_cols=None, scale_cols=None):\n",
        "    # sets of columns\n",
        "    if not drop_cols:\n",
        "        drop_cols = {'id', 'num_private',\n",
        "                    'quality_group',   # exact duplicate of quality\n",
        "                    'recorded_by'}     # uniform value\n",
        "    if not unique_cols:\n",
        "        unique_cols = { 'funder',       # 1898 unique values\n",
        "                        'ward',         # 2092 unique values\n",
        "                        'installer',    # 2146 unique values\n",
        "                        'scheme_name',  # 2697 unique values\n",
        "                        'subvillage',   # 19288 unique values\n",
        "                        'wpt_name' }    # 37400 unique values\n",
        "    if not binary_cols:    \n",
        "        binary_cols = {\"permit\", \"public_meeting\"}\n",
        "    object_cols = set(train.select_dtypes(include=['object']).columns) - drop_cols\n",
        "    if not onehot_cols:\n",
        "        onehot_cols = ((((object_cols | {'region_code', 'district_code'})\n",
        "                    - {'date_recorded', 'permit', 'public_meeting'})\n",
        "                    - unique_cols) - drop_cols)\n",
        "    if not scale_cols:\n",
        "        scale_cols  = (((((set(train.select_dtypes(include=['number']).columns)\n",
        "                    - onehot_cols) - unique_cols) - binary_cols) - drop_cols)\n",
        "                    | {\"year_recorded\"})\n",
        "\n",
        "    # Drop irrelevant/redundant columns\n",
        "    print(f\"dropping: {drop_cols}\")\n",
        "    train = train.drop(drop_cols, axis='columns')\n",
        "    test  = test.drop(drop_cols, axis='columns')\n",
        "\n",
        "    # Onehot Encoding (train labels)\n",
        "    labels = labels.pop('status_group').values\n",
        "    labels = pd.get_dummies(labels)\n",
        "\n",
        "    # Remove NaN's\n",
        "    print(\"removing NaN's:\")\n",
        "    for col in binary_cols:\n",
        "        print(\"\\t\", col)\n",
        "        train[col] = train[col].fillna(False).astype('float64')\n",
        "        test[col] = test[col].fillna(False).astype('float64')\n",
        "\n",
        "    # Replace 0's with mean\n",
        "    print(\"replacing 0's with mean:\")\n",
        "    for col in {\"population\", \"amount_tsh\", \"construction_year\"}:\n",
        "        print(\"\\t\", col)\n",
        "        train = clean_data_with_mean(train,col)\n",
        "        test  = clean_data_with_mean(test,col)\n",
        "    \n",
        "    # Cyclic Encoding\n",
        "    print(\"cyclic encoding:\\n\\tdate_recorded -> (doy_recorded_sin, doy_recorded_cos), year_recorded\")\n",
        "    train = date_to_cyclic(train)\n",
        "    test = date_to_cyclic(test)\n",
        "\n",
        "    # Normalization\n",
        "    print(\"Normalization:\")\n",
        "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    for col in scale_cols:\n",
        "        print(f\"\\t{col} [{min(train[col]), max(train[col])}]\", end=\" -> \")\n",
        "        train[col] = scaler.fit_transform(train[col].to_numpy().reshape(-1,1))\n",
        "        test[col]  = scaler.transform(test[col].to_numpy().reshape(-1,1))\n",
        "        print(f\"[{min(train[col]), max(train[col])}]\")\n",
        "\n",
        "    # Onehot Encoding\n",
        "    train, test = onehot_encode(train, test, onehot_cols)\n",
        "\n",
        "    # Binary Encoding\n",
        "    print(\"Binary Encoding:\")\n",
        "    train, test = nominal_to_binary(train, test, unique_cols)\n",
        "\n",
        "    return train, labels, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qWZVYVAx3S0",
        "colab_type": "text"
      },
      "source": [
        "#### execution of preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phFfzI3u0ppG",
        "colab_type": "code",
        "outputId": "24be892b-3a52-40b1-dc64-d68be1f834a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "train   = pd.read_csv(\"train_set_values.csv\")\n",
        "labels  = pd.read_csv(\"train_set_labels.csv\")\n",
        "test    = pd.read_csv(\"test_values.csv\")\n",
        "\n",
        "train, labels, test = preprocess_data(train, labels, test)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dropping: {'num_private', 'quality_group', 'id', 'recorded_by'}\n",
            "removing NaN's:\n",
            "\t public_meeting\n",
            "\t permit\n",
            "replacing 0's with mean:\n",
            "\t amount_tsh\n",
            "\t population\n",
            "\t construction_year\n",
            "cyclic encoding:\n",
            "\tdate_recorded -> (doy_recorded_sin, doy_recorded_cos), year_recorded\n",
            "Normalization:\n",
            "\tamount_tsh [(0.2, 350000.0)] -> [(-0.3590986010964154, 117.97090065790498)]\n",
            "\tgps_height [(-90, 2770)] -> [(-1.0940495369292693, 3.0322765453176466)]\n",
            "\tlatitude [(-11.64944018, -2e-08)] -> [(-2.0174538540773703, 1.9368783817775395)]\n",
            "\tyear_recorded [(2002, 2013)] -> [(-10.348549589913485, 1.1247289744119464)]\n",
            "\tpopulation [(1.0, 30500.0)] -> [(-0.6199883131902885, 66.89122175181514)]\n",
            "\tconstruction_year [(1960.0, 2013.0)] -> [(-3.6574406124580006, 1.6079677259760634)]\n",
            "\tlongitude [(0.0, 40.34519307)] -> [(-5.188894889753043, 0.9543790152566973)]\n",
            "one-hot encoding:\n",
            "\tpayment_type:\t7 categories\n",
            "\tsource_type:\t7 categories\n",
            "\tpayment:\t7 categories\n",
            "\tquantity:\t5 categories\n",
            "\tregion_code:\t27 categories\n",
            "\tsource_class:\t3 categories\n",
            "\twaterpoint_type:\t7 categories\n",
            "\textraction_type_group:\t13 categories\n",
            "\tmanagement_group:\t5 categories\n",
            "\tmanagement:\t12 categories\n",
            "\tsource:\t10 categories\n",
            "\tquantity_group:\t5 categories\n",
            "\textraction_type_class:\t7 categories\n",
            "\textraction_type:\t18 categories\n",
            "\tbasin:\t9 categories\n",
            "\twater_quality:\t8 categories\n",
            "\twaterpoint_type_group:\t6 categories\n",
            "\tscheme_management:\t13 categories\n",
            "\tlga:\t125 categories\n",
            "\tdistrict_code:\t20 categories\n",
            "\tregion:\t21 categories\n",
            "Binary Encoding:\n",
            "binary encoding:\n",
            "\t ward \t 2092  unique values  -> 12  columns\n",
            "\t subvillage \t 19288  unique values  -> 15  columns\n",
            "\t installer \t 2146  unique values  -> 12  columns\n",
            "\t scheme_name \t 2697  unique values  -> 12  columns\n",
            "\t funder \t 1898  unique values  -> 11  columns\n",
            "\t wpt_name \t 37400  unique values  -> 16  columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_vRRDSaJIic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = train.to_numpy()\n",
        "y = labels.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqW4K2fE2M9k",
        "colab_type": "text"
      },
      "source": [
        "#### Old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e1kLX002LFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do_not_include = {'quality_group',      # exact duplicate of 'quality'\n",
        "                  'recorded_by',        # uniform value\n",
        "\n",
        "                  'wpt_name',           #unique\n",
        "                  'subvillage',         #dup of loc, numerous\n",
        "                  'scheme_name',        #messy numerous\n",
        "                  'payment',            #duplicate of payment type\n",
        "                  'quantity_group',     #Dup of quantity\n",
        "                  'waterpoint_type_group',#dup of wtptype less data\n",
        "                  'source_type',        #dup of source\n",
        "                  'extraction_type',    #dup of extr_type_group\n",
        "                  #'extraction_type_group',\n",
        "                  'region'} - unique_cols#enc in regcode\n",
        "\n",
        "#These are sort of ordinal data\n",
        "do_not_one_hot = {'id','gps_height','doy_recorded_sin','doy_recorded_cos',\n",
        "                  'longitude','latitude','population','amount_tsh'} | unique_cols\n",
        "\n",
        "clean_up = {'population','amount_tsh'}\n",
        "\n",
        "#Tentatively do not include.\n",
        "do_not_include_tent = {'funder','installer','ward','lga'} - unique_cols\n",
        "\n",
        "#Don't know what it means \n",
        "do_not_include_temp = {'num_private'} - unique_cols\n",
        "\n",
        "\n",
        "x, train_col,y = load_train(\"train_set_values.csv\",\"train_set_labels.csv\",\n",
        "                            do_not_include, do_not_one_hot,\n",
        "                            clean_up, do_not_include_tent, do_not_include_temp,\n",
        "                            unique_cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8vnqwhZ2O3M",
        "colab_type": "text"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xdZr_LgOZfY",
        "colab_type": "code",
        "outputId": "3c2eb0da-f8d9-47c7-a213-174ad642ac0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "###Model\n",
        "\n",
        "hidden = 1024\n",
        "dropout = 0.4\n",
        "\n",
        "(trash, input_dim) = x.shape\n",
        "\n",
        "activation = 'relu'\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Dense(hidden, input_dim = input_dim))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Activation(activation))\n",
        "\n",
        "model.add(Dense(hidden, input_dim = input_dim))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Activation(activation))\n",
        "\n",
        "model.add(Dense(hidden, input_dim = input_dim))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Activation(activation))\n",
        "\n",
        "\n",
        "model.add(Dense(3,input_dim = hidden))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "#history = model.fit(x, y, epochs=20, batch_size=4096*16, validation_split=0.2)\n",
        "history = model.fit(x, y, epochs=20, batch_size=256, validation_split=0.2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 47520 samples, validate on 11880 samples\n",
            "Epoch 1/20\n",
            "47520/47520 [==============================] - 4s 88us/step - loss: 0.6573 - acc: 0.7257 - val_loss: 0.5919 - val_acc: 0.7644\n",
            "Epoch 2/20\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.5731 - acc: 0.7649 - val_loss: 0.5503 - val_acc: 0.7775\n",
            "Epoch 3/20\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.5303 - acc: 0.7818 - val_loss: 0.5390 - val_acc: 0.7836\n",
            "Epoch 4/20\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.5036 - acc: 0.7931 - val_loss: 0.5343 - val_acc: 0.7792\n",
            "Epoch 5/20\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.4801 - acc: 0.8007 - val_loss: 0.5153 - val_acc: 0.7916\n",
            "Epoch 6/20\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.4603 - acc: 0.8094 - val_loss: 0.5151 - val_acc: 0.7932\n",
            "Epoch 7/20\n",
            "47520/47520 [==============================] - 4s 78us/step - loss: 0.4404 - acc: 0.8181 - val_loss: 0.5185 - val_acc: 0.7893\n",
            "Epoch 8/20\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.4208 - acc: 0.8250 - val_loss: 0.5174 - val_acc: 0.7928\n",
            "Epoch 9/20\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.4055 - acc: 0.8313 - val_loss: 0.5320 - val_acc: 0.7862\n",
            "Epoch 10/20\n",
            "47520/47520 [==============================] - 4s 76us/step - loss: 0.3897 - acc: 0.8380 - val_loss: 0.5229 - val_acc: 0.7956\n",
            "Epoch 11/20\n",
            "47520/47520 [==============================] - 4s 77us/step - loss: 0.3756 - acc: 0.8438 - val_loss: 0.5323 - val_acc: 0.7952\n",
            "Epoch 12/20\n",
            "47520/47520 [==============================] - 4s 75us/step - loss: 0.3632 - acc: 0.8487 - val_loss: 0.5315 - val_acc: 0.7949\n",
            "Epoch 13/20\n",
            "39424/47520 [=======================>......] - ETA: 0s - loss: 0.3449 - acc: 0.8550"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-6c561411f0b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#history = model.fit(x, y, epochs=20, batch_size=4096*16, validation_split=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ2-AmxsWhSf",
        "colab_type": "code",
        "outputId": "2d900d33-6bd2-467d-cca3-f9d1f50cac0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "max_score = -1\n",
        "iter_of_max = 0,0\n",
        "train_plot = []\n",
        "val_plot = []\n",
        "x_axis = []\n",
        "\n",
        "#Save the weights for k-fold validation\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "amount = 120\n",
        "k_folds = 5\n",
        "\n",
        "###When i use k_folds instead of 10 for the np.zeros initialization it acts up\n",
        "val_plot = np.zeros((k_folds,amount))\n",
        "train_plot = np.zeros((k_folds,amount))\n",
        "test_plot = np.zeros((k_folds,amount)) \n",
        "x_axis = np.zeros((k_folds,amount))\n",
        "total = int(x.shape[0])\n",
        "for j in range(k_folds):\n",
        "\n",
        "    whole = np.arange(0,total)\n",
        "    test_range = np.arange((j)*(total/k_folds), (j)*(total/k_folds)+(total/k_folds),dtype = 'int')\n",
        "    \n",
        "    train_range = np.delete(whole, test_range)\n",
        "    \n",
        "    \n",
        "    #K-fold validation setup\n",
        "    x_train = x[train_range,:]\n",
        "    x_pretest = x[test_range,:]\n",
        "    y_train = y[train_range,:]\n",
        "    y_pretest = y[test_range,:]\n",
        "    \n",
        "    for i in range(amount):\n",
        "\n",
        "        history = model.fit(x_train, y_train, epochs = 1, batch_size = 4096*16, verbose = 0)\n",
        "        score = model.evaluate(x_pretest, y_pretest, batch_size = 512, verbose = 0)\n",
        "\n",
        "        if  float(100 * score[1]) > float(max_score):\n",
        "            max_score = float(100 * score[1])\n",
        "            iter_of_max = i,j\n",
        "        if i%10 == 0:\n",
        "            print(\"----------\",i,\"-\",j,\"-----------\")\n",
        "            print(\"Test accuracy: \", (100.0 * score[1]))\n",
        "            print(\"Maxscore: \", max_score, \"at\", iter_of_max,\"epoch-kthfold\")\n",
        "        \n",
        "        #for the plot\n",
        "        x_axis[j,i] = i\n",
        "        test_plot[j,i] = (score[1])\n",
        "        train_plot[j,i] = np.array(history.history['acc'])\n",
        "\n",
        "        \n",
        "\n",
        "    if j == k_folds - 1:\n",
        "        break\n",
        "    model.load_weights('model.h5')\n",
        "    print(\"Reloading Model\")\n",
        "    \n",
        "print(\"Done evaluating performance\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- 0 - 0 -----------\n",
            "Test accuracy:  54.23400671393783\n",
            "Maxscore:  54.23400671393783 at (0, 0) epoch-kthfold\n",
            "---------- 10 - 0 -----------\n",
            "Test accuracy:  67.4242424081873\n",
            "Maxscore:  67.74410774009397 at (9, 0) epoch-kthfold\n",
            "---------- 20 - 0 -----------\n",
            "Test accuracy:  71.74242421834155\n",
            "Maxscore:  71.74242421834155 at (20, 0) epoch-kthfold\n",
            "---------- 30 - 0 -----------\n",
            "Test accuracy:  73.41750840546707\n",
            "Maxscore:  73.80471382478271 at (28, 0) epoch-kthfold\n",
            "---------- 40 - 0 -----------\n",
            "Test accuracy:  75.06734007536762\n",
            "Maxscore:  75.06734007536762 at (40, 0) epoch-kthfold\n",
            "---------- 50 - 0 -----------\n",
            "Test accuracy:  75.90067340870095\n",
            "Maxscore:  75.90067340870095 at (50, 0) epoch-kthfold\n",
            "---------- 60 - 0 -----------\n",
            "Test accuracy:  76.82659933462689\n",
            "Maxscore:  76.95286196088952 at (59, 0) epoch-kthfold\n",
            "---------- 70 - 0 -----------\n",
            "Test accuracy:  77.66835018039151\n",
            "Maxscore:  77.66835018039151 at (70, 0) epoch-kthfold\n",
            "---------- 80 - 0 -----------\n",
            "Test accuracy:  77.76094274087386\n",
            "Maxscore:  77.76094274087386 at (80, 0) epoch-kthfold\n",
            "---------- 90 - 0 -----------\n",
            "Test accuracy:  77.97138049546317\n",
            "Maxscore:  77.99663302071568 at (86, 0) epoch-kthfold\n",
            "---------- 100 - 0 -----------\n",
            "Test accuracy:  77.3316498196085\n",
            "Maxscore:  78.19865319062563 at (97, 0) epoch-kthfold\n",
            "---------- 110 - 0 -----------\n",
            "Test accuracy:  78.1060605859917\n",
            "Maxscore:  78.28282828282829 at (102, 0) epoch-kthfold\n",
            "Reloading Model\n",
            "---------- 0 - 1 -----------\n",
            "Test accuracy:  54.19191916783651\n",
            "Maxscore:  78.28282828282829 at (102, 0) epoch-kthfold\n",
            "---------- 10 - 1 -----------\n",
            "Test accuracy:  68.49326599727978\n",
            "Maxscore:  78.28282828282829 at (102, 0) epoch-kthfold\n",
            "---------- 20 - 1 -----------\n",
            "Test accuracy:  73.41750840948086\n",
            "Maxscore:  78.28282828282829 at (102, 0) epoch-kthfold\n",
            "---------- 30 - 1 -----------\n",
            "Test accuracy:  74.88215486208598\n",
            "Maxscore:  78.28282828282829 at (102, 0) epoch-kthfold\n",
            "---------- 40 - 1 -----------\n",
            "Test accuracy:  75.68181816174928\n",
            "Maxscore:  78.28282828282829 at (102, 0) epoch-kthfold\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-b50c0257cb2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pretest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pretest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m  \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1298\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                                          \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m                                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m     def predict(self, x,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}